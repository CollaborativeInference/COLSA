---
title: "COLSA pacakge"
subtitle: "A package for Privacy enhanced collaborative inference in the Cox proportional hazards model for distributed data"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This is a tutorial for using the COLSA package designed for analyzing distributed survival data.

# Install Pacakges
```{r}
devtools::load_all() 
library(survival)
```
# simulmixweibull() : simulate datasets
In the simulation, we considered $2$ continuous covariates and $2$ categorical covaraites. The continuous covariates are generated from a bivariate normal distribution. The first categorical covariate is generated from a Bernoulli distribution and the second categorical with $4$ classes is generated from a multinomial distribution with probabilities depending on the two levels of the first categorical covariate. The event times are generated from a mixture of Weibull distributions with shape parameters $3$ and $5$ and scale parameters $10$ and $20$. The censoring times are generated from an exponential distribution with rate $3$. The event times are right-censored at the censoring times. The true regression coefficients are $\begin{bmatrix} 0.15 & -0.15 & 0.3 & 0.3 & 0.3 & 0.3 \end{bmatrix}$. We aimed to generate $6$ datasets with $1500$ observations in the first $3$ datasets and $500$ observations in the last $3$ datasets. We then randomly split the generated data into the $6$ corresponding datasets. 
```{r}
ns = c(1500,1500,1500,500,500,500)
K = length(ns)
data = simulmixweibull(N = sum(ns),lambdas =  c(10, 20), gammas =  c(3,5),beta=c(0.15,-0.15,0.3,0.3,0.3,0.3), rateC=3)

```
Randomly split the generated data into 6 datasets.
```{r}
group_indicator = c()
for(k in c(1:(K))){
    group_indicator= c(group_indicator,rep(k,ns[k]))
  }
# The CoxPH regression formula  
form =as.formula(paste("Surv(time, status)", paste(names(data)[4:length(names(data))],collapse="+"), sep = " ~ "))
data$group = group_indicator
data$order = data$group
```
# A quick demo using updateCOLSA.demo on the generated data
Approximate the log baseline hazards using $4$th order Bernstein polynomial. Find initial estimates for gamma and beta using the first dataset.
```{r}
data_first = subset(data,order==1)
dg = 4
boundaryknots = c(0,max(data$time))
initial_val = find_inits(dg,data_first,form,boundaryknots)

```
Obtain the COLSA estimate using the example data.
```{r}
res = updateCOLSA.demo(K,data,initial_val,6+dg+1,6,boundaryknots)
```

# Applying updateCOLSA on the distributed data
In a real-world application, each hospital(analysis center) only needs to store their data in their local facility. To mimic the set-up, we saved each data set into separate folders. 
```{r}
tempdatadir = getwd()
save_data(tempdatadir,6)
```
Load the subdata from site 1.  We wil use the same `find_inits` function to find initial values and we initialize the Hessian Matrix to be all zeros (NA). We also set the negative log likelihood to be zero.
```{r}
b = 1
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
dg = 4
boundaryknots = c(0,max(data$time))
initial_val = find_inits(dg,subdata,form,boundaryknots)
res = updateCOLSA.outloop(subdata,list(betahat=initial_val,Hessian = NA,negl= 0),6+dg+1,6,boundaryknots)
res_site1 = res$result
summary_stat_site1 = res$statistics
print(summary_stat_site1)
```
Once site 1 finishes analysis,the summary statistics `summary_stat_site1` will be passed on to Site 2. Site 2 has the turn the update the results.

```{r}
b = 2
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
summary_stat_site2 = updateCOLSA.outloop(subdata,summary_stat_site1,6+dg+1,6,boundaryknots)$statistics

```
The process goes on till the $6$th site.

```{r}
b = 3
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
summary_stat_site3 = updateCOLSA.outloop(subdata,summary_stat_site2,6+dg+1,6,boundaryknots)$statistics
b = 4
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
summary_stat_site4 = updateCOLSA.outloop(subdata,summary_stat_site3,6+dg+1,6,boundaryknots)$statistics
b = 5
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
summary_stat_site5 = updateCOLSA.outloop(subdata,summary_stat_site4,6+dg+1,6,boundaryknots)$statistics
b = 6
load(paste(tempdatadir,"/Simdata/hospital",b,"/Simdata.RData",sep=""))
res6 = updateCOLSA.outloop(subdata,summary_stat_site5,6+dg+1,6,boundaryknots)
print(res6$result)

```
# Plot the estimated survival curve.
Once we obtained the COLSA estimate, we can construct the survival curves for any covariates values. Here we plot the baseline survival curve and compare it with the true baseline survival curves.

```{r}
library(ggplot2)
t_plot = seq(0,max(data$time[data$status==1]),0.001)
lambda =  c(10, 20); gamma =  c(3,5)
true_surv = 0.5*exp(-lambda[1]*t_plot^(gamma[1]))+  (1-0.5)*exp(-lambda[2]*t_plot^(gamma[2]))
colsa_surv = get_est_surv(res6$result[,1],t_plot,matrix(0,1,6),boundaryknots)
true_surv_df = data.frame(time =t_plot, surv = true_surv,method = "ground-truth")
colsa_surv_df =  data.frame(time = t_plot, surv = colsa_surv ,method = "COLSA")
ggplot(NULL, aes(time,surv,method)) +
    geom_point(data=true_surv_df,aes(color=method),size=0.1)+
    geom_point(data=colsa_surv_df,aes(color = method),size=0.1)

```
